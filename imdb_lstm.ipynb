{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Loading and Pre-processing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data(num_words, sequence_length, test_size=0.25, oov_token=None):\n",
    "    # read reviews\n",
    "    reviews = []\n",
    "    with open(\"data/reviews.txt\",encoding=\"utf8\") as f:\n",
    "        for review in f:\n",
    "            review = review.strip()\n",
    "            reviews.append(review)\n",
    "\n",
    "    labels = []\n",
    "    with open(\"data/labels.txt\",encoding=\"utf8\") as f:\n",
    "        for label in f:\n",
    "            label = label.strip()\n",
    "            labels.append(label)\n",
    "    # tokenize the dataset corpus, delete uncommon words such as names, etc.\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(reviews)\n",
    "    X = tokenizer.texts_to_sequences(reviews)\n",
    "    X, y = np.array(X), np.array(labels)\n",
    "    # pad sequences with 0's\n",
    "    X = pad_sequences(X, maxlen=sequence_length)\n",
    "    # convert labels to one-hot encoded\n",
    "    y = to_categorical(y)\n",
    "    # split data to training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=1)\n",
    "    data = {}\n",
    "    data[\"X_train\"] = X_train\n",
    "    data[\"X_test\"]= X_test\n",
    "    data[\"y_train\"] = y_train\n",
    "    data[\"y_test\"] = y_test\n",
    "    data[\"tokenizer\"] = tokenizer\n",
    "    data[\"int2label\"] =  {0: \"negative\", 1: \"positive\"}\n",
    "    data[\"label2int\"] = {\"negative\": 0, \"positive\": 1}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<h2> Embeddings using GloVe pre trained model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vectors(word_index, embedding_size=100):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_size))\n",
    "    with open(f\"data/glove.6B.{embedding_size}d.txt\", encoding=\"utf8\") as f:\n",
    "        for line in tqdm(f, \"Reading GloVe\"):\n",
    "            values = line.split()\n",
    "            # get the word as the first word in the line\n",
    "            word = values[0]\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                # get the vectors as the remaining values in the line\n",
    "                embedding_matrix[idx] = np.array(values[1:], dtype=\"float32\")\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(word_index, units=128, n_layers=1, cell=LSTM, bidirectional=False,\n",
    "                embedding_size=100, sequence_length=100, dropout=0.3, \n",
    "                loss=\"categorical_crossentropy\", optimizer=\"adam\", \n",
    "                output_length=2):\n",
    "    \"\"\"Constructs a RNN model given its parameters\"\"\"\n",
    "    embedding_matrix = get_embedding_vectors(word_index, embedding_size)\n",
    "    model = Sequential()\n",
    "    # add the embedding layer\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "              embedding_size,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              input_length=sequence_length))\n",
    "    for i in range(n_layers):\n",
    "        if i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # first layer or hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(output_length, activation=\"softmax\"))\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of words in each sentence\n",
    "SEQUENCE_LENGTH = 300\n",
    "# N-Dimensional GloVe embedding vectors\n",
    "EMBEDDING_SIZE = 300\n",
    "# number of words to use, discarding the rest\n",
    "N_WORDS = 10000\n",
    "# out of vocabulary token\n",
    "OOV_TOKEN = None\n",
    "# 30% testing set, 70% training set\n",
    "TEST_SIZE = 0.3\n",
    "# number of CELL layers\n",
    "N_LAYERS = 1\n",
    "# the RNN cell to use, LSTM in this case\n",
    "RNN_CELL = LSTM\n",
    "# whether it's a bidirectional RNN\n",
    "IS_BIDIRECTIONAL = False\n",
    "# number of units (RNN_CELL ,nodes) in each layer\n",
    "UNITS = 128\n",
    "# dropout rate\n",
    "DROPOUT = 0.4\n",
    "### Training parameters\n",
    "LOSS = \"categorical_crossentropy\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 6\n",
    "\n",
    "def get_model_name(dataset_name):\n",
    "    # construct the unique model name\n",
    "    model_name = f\"{dataset_name}-{RNN_CELL.__name__}-seq-{SEQUENCE_LENGTH}-em-{EMBEDDING_SIZE}-w-{N_WORDS}-layers-{N_LAYERS}-units-{UNITS}-opt-{OPTIMIZER}-BS-{BATCH_SIZE}-d-{DROPOUT}\"\n",
    "    if IS_BIDIRECTIONAL:\n",
    "        # add 'bid' str if bidirectional\n",
    "        model_name = \"bid-\" + model_name\n",
    "    if OOV_TOKEN:\n",
    "        # add 'oov' str if OOV token is specified\n",
    "        model_name += \"-oov\"\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading GloVe: 400000it [00:41, 9571.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 300)          37267200  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 37,487,106\n",
      "Trainable params: 219,906\n",
      "Non-trainable params: 37,267,200\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "  1/547 [..............................] - ETA: 0s - loss: 0.6951 - accuracy: 0.5469WARNING:tensorflow:From C:\\Users\\Ketaki Barde\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "  2/547 [..............................] - ETA: 11:32 - loss: 0.7132 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.6983s vs `on_train_batch_end` time: 1.8458s). Check your callbacks.\n",
      "547/547 [==============================] - 394s 719ms/step - loss: 0.4793 - accuracy: 0.7729 - val_loss: 0.4450 - val_accuracy: 0.8225\n",
      "Epoch 2/6\n",
      "547/547 [==============================] - 424s 775ms/step - loss: 0.3188 - accuracy: 0.8663 - val_loss: 0.3089 - val_accuracy: 0.8723\n",
      "Epoch 3/6\n",
      "547/547 [==============================] - 434s 794ms/step - loss: 0.2654 - accuracy: 0.8909 - val_loss: 0.2653 - val_accuracy: 0.8916\n",
      "Epoch 4/6\n",
      "547/547 [==============================] - 358s 655ms/step - loss: 0.2376 - accuracy: 0.9038 - val_loss: 0.2556 - val_accuracy: 0.8931\n",
      "Epoch 5/6\n",
      "547/547 [==============================] - 317s 579ms/step - loss: 0.2087 - accuracy: 0.9152 - val_loss: 0.2968 - val_accuracy: 0.8781\n",
      "Epoch 6/6\n",
      "547/547 [==============================] - 336s 614ms/step - loss: 0.1862 - accuracy: 0.9272 - val_loss: 0.2368 - val_accuracy: 0.9073\n"
     ]
    }
   ],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "# load the data\n",
    "data = load_imdb_data(N_WORDS, SEQUENCE_LENGTH, TEST_SIZE, oov_token=OOV_TOKEN)\n",
    "dataset_name = \"imdb\"\n",
    "model_name = get_model_name(dataset_name)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(data[\"tokenizer\"].word_index, units=UNITS, n_layers=N_LAYERS, \n",
    "                    cell=RNN_CELL, bidirectional=IS_BIDIRECTIONAL, embedding_size=EMBEDDING_SIZE, \n",
    "                    sequence_length=SEQUENCE_LENGTH, dropout=DROPOUT, \n",
    "                    loss=LOSS, optimizer=OPTIMIZER, output_length=data[\"y_train\"][0].shape[0])\n",
    "model.summary()\n",
    "\n",
    "# using tensorboard on 'logs' folder\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# start training\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[tensorboard],\n",
    "                    verbose=1)\n",
    "# save the resulting model into 'results' folder\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(text):\n",
    "    sequence = data[\"tokenizer\"].texts_to_sequences([text])\n",
    "    # pad the sequences\n",
    "    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
    "    # get the prediction\n",
    "    prediction = model.predict(sequence)[0]\n",
    "    return prediction, data[\"int2label\"][np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output vector: [0.49692515 0.5030748 ]\n",
      "Prediction: positive\n"
     ]
    }
   ],
   "source": [
    "text = \"The movie is awesome!\"\n",
    "output_vector, prediction = get_predictions(text)\n",
    "print(\"Output vector:\", output_vector)\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output vector: [0.9348459  0.06515405]\n",
      "Prediction: negative\n"
     ]
    }
   ],
   "source": [
    "text = \"The movie is bad.\"\n",
    "output_vector, prediction = get_predictions(text)\n",
    "print(\"Output vector:\", output_vector)\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output vector: [0.4290174 0.5709826]\n",
      "Prediction: positive\n"
     ]
    }
   ],
   "source": [
    "text = \"Not very good, but pretty good try.\"\n",
    "output_vector, prediction = get_predictions(text)\n",
    "print(\"Output vector:\", output_vector)\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
